import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import os
import streamlit as st

# 1. Load Excel files
folder_path_jobs = "excel_files"  
folder_path_personnel = "personal_id/2.xlsx" 
final_scores_file = "personal_id/6.xlsx" 

# Load job data
all_jobs = []
for file_name in os.listdir(folder_path_jobs):
    if file_name.endswith(".xlsx"):
        file_path = os.path.join(folder_path_jobs, file_name)
        df = pd.read_excel(file_path)
        if "Ø³Ù…Øª" in df.columns and "Ø´Ø§Ø®Øµ" in df.columns:
            df = df[["Ø³Ù…Øª", "Ø´Ø§Ø®Øµ"]]
            all_jobs.append(df)

# Combine all job data
if all_jobs:
    jobs_df = pd.concat(all_jobs, ignore_index=True)
else:
    st.error("Ù‡ÛŒÚ† ÙØ§ÛŒÙ„ Ø´ØºÙ„ÛŒ Ù…Ø¹ØªØ¨Ø±ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯.")
    st.stop()

# Load personnel data
all_personnel = []
df = pd.read_excel(folder_path_personnel)
if "Ú©Ø¯ Ù¾Ø±Ø³Ù†Ù„ÛŒ" in df.columns and "Ø´Ø§Ø®Øµ" in df.columns:
    df = df[["Ú©Ø¯ Ù¾Ø±Ø³Ù†Ù„ÛŒ", "Ø´Ø§Ø®Øµ"]]
    all_personnel.append(df)

# Combine all personnel data
if all_personnel:
    personnel_df = pd.concat(all_personnel, ignore_index=True)
else:
    st.error("error while uploading files")
    st.stop()

# Remove duplicate personnel codes and criteria
personnel_df = personnel_df.drop_duplicates(subset=["Ú©Ø¯ Ù¾Ø±Ø³Ù†Ù„ÛŒ", "Ø´Ø§Ø®Øµ"])

# Load final scores data
if os.path.exists(final_scores_file):
    final_scores_df = pd.read_excel(final_scores_file)
    if {"Ú©Ø¯ Ù¾Ø±Ø³Ù†Ù„ÛŒ", "Ù†Ù…Ø±Ù‡ Ù†Ù‡Ø§ÛŒÛŒ"}.issubset(final_scores_df.columns):
        # Calculate the mean score for each personnel code
        final_scores_mean = final_scores_df.groupby("Ú©Ø¯ Ù¾Ø±Ø³Ù†Ù„ÛŒ")["Ù†Ù…Ø±Ù‡ Ù†Ù‡Ø§ÛŒÛŒ"].mean().reset_index()
        final_scores_mean = final_scores_mean.rename(columns={"Ù†Ù…Ø±Ù‡": "Ù†Ù…Ø±Ù‡ Ù†Ù‡Ø§ÛŒÛŒ"})
    else:
        st.error("error while uploading files")
        st.stop()
else:
    st.error("error while uploading files")
    st.stop()

# 2. Vectorize criteria
vectorizer = TfidfVectorizer()
jobs_X = vectorizer.fit_transform(jobs_df["Ø´Ø§Ø®Øµ"])
personnel_X = vectorizer.transform(personnel_df["Ø´Ø§Ø®Øµ"])

# Set up the UI
st.set_page_config(page_title="Ø³ÛŒØ³ØªÙ… Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ø´ØºÙ„", layout="wide")
st.title("ğŸ¯ Ø³ÛŒØ³ØªÙ… Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ ÙØ±Ø¯ Ù…Ù†Ø§Ø³Ø¨")
st.markdown(".Ø§ÛŒÙ† Ø§Ø¨Ø²Ø§Ø± Ø¨Ù‡ Ø´Ù…Ø§ Ú©Ù…Ú© Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ø§ÙØ±Ø§Ø¯ Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯Ù†Ø¸Ø± Ø±Ø§ Ù¾ÛŒØ¯Ø§ Ú©Ù†ÛŒØ¯.")

# CSS for right-to-left alignment
st.markdown(
    """
    <style>
    body {
        direction: rtl;
        text-align: right;
    }
    </style>
    """,
    unsafe_allow_html=True
)

# Get user input
st.markdown("#### Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§ÛŒ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ø§ Ú©Ø§Ù…Ø§ (,) Ø§Ø² Ù‡Ù… Ø¬Ø¯Ø§ Ú©Ù†ÛŒØ¯:")
user_input = st.text_input("Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§:", "")
if user_input:
    # Convert input into a list
    input_list = [item.strip() for item in user_input.split(",")]
    user_vector = vectorizer.transform([", ".join(input_list)])

    # Compute similarities
    similarity_scores = cosine_similarity(user_vector, personnel_X)[0]

    # Add similarity scores to personnel data
    personnel_df["Ø§Ù…ØªÛŒØ§Ø² Ø´Ø¨Ø§Ù‡Øª"] = similarity_scores

    # Remove duplicate personnel codes
    personnel_df = personnel_df.drop_duplicates(subset=["Ú©Ø¯ Ù¾Ø±Ø³Ù†Ù„ÛŒ"])

    # Merge with final scores data
    result_df = pd.merge(personnel_df, final_scores_mean, on="Ú©Ø¯ Ù¾Ø±Ø³Ù†Ù„ÛŒ", how="left")

    # Sort by similarity score and final score
    result_df = result_df.sort_values(by=["Ø§Ù…ØªÛŒØ§Ø² Ø´Ø¨Ø§Ù‡Øª", "Ù†Ù…Ø±Ù‡ Ù†Ù‡Ø§ÛŒÛŒ"], ascending=[False, False]).head(10)

    # Display results
    st.markdown("### âœ… Ø§ÙØ±Ø§Ø¯ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ:")
    for index, row in result_df.iterrows():
        st.success(
            f"**Ú©Ø¯ Ù¾Ø±Ø³Ù†Ù„ÛŒ: {row['Ú©Ø¯ Ù¾Ø±Ø³Ù†Ù„ÛŒ']}** "
            f"(Ø§Ù…ØªÛŒØ§Ø² Ø´Ø¨Ø§Ù‡Øª: {row['Ø§Ù…ØªÛŒØ§Ø² Ø´Ø¨Ø§Ù‡Øª']:.2f}, Ù†Ù…Ø±Ù‡ Ù†Ù‡Ø§ÛŒÛŒ: {row['Ù†Ù…Ø±Ù‡ Ù†Ù‡Ø§ÛŒÛŒ']:.2f})"
        )
